{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part1: Data Pre-Processing",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cdOCTNqZBcbp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "78be5308-841d-458c-d3ca-e83602b9d9ad"
      },
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.11.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "metadata": {
        "id": "wmxicW9BBnNt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Part 1: Data Pre-Processing**\n",
        "\n",
        "NLP is a filed of machine learning with the ability of a computer to understand, analyse, manipulate and potentially generate human language."
      ]
    },
    {
      "metadata": {
        "id": "Kd94gZmxBmeT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Steps in Data Pre-Processing :"
      ]
    },
    {
      "metadata": {
        "id": "V8KxxKjTCf3E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step 1: Convert text to lowercase **"
      ]
    },
    {
      "metadata": {
        "id": "ogQbi28gCapN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e9a6ff2b-14ab-4130-be71-532103c740bb"
      },
      "cell_type": "code",
      "source": [
        "input_str = \"The 5 biggest countries by population in 2017 are China, India, United States, Indonesia, and Brazil.\"\n",
        "input_str = input_str.lower()\n",
        "print(\"Lower case o/p: \",input_str)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lower case o/p:  the 5 biggest countries by population in 2017 are china, india, united states, indonesia, and brazil.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g_isRTJhDFsU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step 2: Remove numbers **"
      ]
    },
    {
      "metadata": {
        "id": "WP5wjWMnCtC5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1f314df9-cdda-4810-8d70-e11d7733bfd9"
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "input_str = \"Box A contains 3 red and 5 white balls, while Box B contains 4 red and 2 blue balls.\"\n",
        "result = re.sub(r'\\d+', '', input_str)\n",
        "print(result)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Box A contains  red and  white balls, while Box B contains  red and  blue balls.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zTQeVNDpDgPa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step 2: Remove punctuation **"
      ]
    },
    {
      "metadata": {
        "id": "17_vvDluDOmW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e5dbb34a-e015-48ba-b367-80aaed4b7d82"
      },
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "result = tokenizer.tokenize('hey! how are you ? buddy')\n",
        "\n",
        "print(result)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hey', 'how', 'are', 'you', 'buddy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JezpE-FJGovr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step 3: Remove StopWords **"
      ]
    },
    {
      "metadata": {
        "id": "Xchfty4pEZq4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "21d322b1-dce7-4e13-83cc-efd2c912c4ed"
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "  \n",
        "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
        "  \n",
        "stop_words = set(stopwords.words('english')) \n",
        "  \n",
        "word_tokens = word_tokenize(example_sent) \n",
        "  \n",
        "filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
        "  \n",
        "print(filtered_sentence) "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3yCo0JbRJL1b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step 4: Tokenization **"
      ]
    },
    {
      "metadata": {
        "id": "XSzShzmDGf8b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f7a11783-a68c-4b75-af8c-a585f974571e"
      },
      "cell_type": "code",
      "source": [
        "input_str = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokens = word_tokenize(input_str)\n",
        "print(tokens)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kA-5suDvJslC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step 5: Stemming **"
      ]
    },
    {
      "metadata": {
        "id": "W6OVegFuJ6GW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "31ccfc8f-17fd-445a-fe07-baf5e623f9f3"
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "stemmer= PorterStemmer()\n",
        "input_str=\"There are several types of stemming algorithms.\"\n",
        "input_str=word_tokenize(input_str)\n",
        "for word in input_str:\n",
        "    print(stemmer.stem(word))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "there\n",
            "are\n",
            "sever\n",
            "type\n",
            "of\n",
            "stem\n",
            "algorithm\n",
            ".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5wSRS2PaJG3c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step 6: Lemmatization **"
      ]
    },
    {
      "metadata": {
        "id": "DbhEYR_tKIdu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "bafe2129-7a6d-4900-8a0a-4c223813a856"
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "input_str=\"been had done languages cities mice\"\n",
        "input_str=word_tokenize(input_str)\n",
        "for word in input_str:\n",
        "    print(lemmatizer.lemmatize(word))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "been\n",
            "had\n",
            "done\n",
            "language\n",
            "city\n",
            "mouse\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LfwPdI7YKvJI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step 7: Part of speech tagging (POS) **"
      ]
    },
    {
      "metadata": {
        "id": "EFlpdx9mKbdz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "11691319-5a38-4a42-bd38-9a02a8d4f9b0"
      },
      "cell_type": "code",
      "source": [
        "input_str=\"Parts of speech examples: an article, to write, interesting, easily, and, of\"\n",
        "from textblob import TextBlob\n",
        "result = TextBlob(input_str)\n",
        "print(result.tags)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[('Parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('examples', 'NNS'), ('an', 'DT'), ('article', 'NN'), ('to', 'TO'), ('write', 'VB'), ('interesting', 'VBG'), ('easily', 'RB'), ('and', 'CC'), ('of', 'IN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "196RIyvuMZrs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step 8: Chunking **"
      ]
    },
    {
      "metadata": {
        "id": "hRsIcYF7K7Ih",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "aca3d1ec-e746-4354-8e60-87eff448f651"
      },
      "cell_type": "code",
      "source": [
        "# step1 : do POS \n",
        "input_str=\"A black television and a white stove were bought for the new apartment of John.\"\n",
        "from textblob import TextBlob\n",
        "result = TextBlob(input_str)\n",
        "print(result.tags)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('A', 'DT'), ('black', 'JJ'), ('television', 'NN'), ('and', 'CC'), ('a', 'DT'), ('white', 'JJ'), ('stove', 'NN'), ('were', 'VBD'), ('bought', 'VBN'), ('for', 'IN'), ('the', 'DT'), ('new', 'JJ'), ('apartment', 'NN'), ('of', 'IN'), ('John', 'NNP')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6C8QlczGMi3Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "4033da3e-ae98-4cd7-a14f-776adaad7e5d"
      },
      "cell_type": "code",
      "source": [
        "# step 2 :chunking\n",
        "\n",
        "reg_exp = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "rp = nltk.RegexpParser(reg_exp)\n",
        "result = rp.parse(result.tags)\n",
        "print(result)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (NP A/DT black/JJ television/NN)\n",
            "  and/CC\n",
            "  (NP a/DT white/JJ stove/NN)\n",
            "  were/VBD\n",
            "  bought/VBN\n",
            "  for/IN\n",
            "  (NP the/DT new/JJ apartment/NN)\n",
            "  of/IN\n",
            "  John/NNP)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gri1JoMuM2jp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step 9: Named-entity recognition **"
      ]
    },
    {
      "metadata": {
        "id": "F1R39ZpRMuM3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "8128b283-5b5f-4ab2-9418-fabf467bfbde"
      },
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "input_str = \"Bill works for Apple so he went to Boston for a conference.\"\n",
        "print(ne_chunk(pos_tag(word_tokenize(input_str))))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "(S\n",
            "  (PERSON Bill/NNP)\n",
            "  works/VBZ\n",
            "  for/IN\n",
            "  Apple/NNP\n",
            "  so/IN\n",
            "  he/PRP\n",
            "  went/VBD\n",
            "  to/TO\n",
            "  (GPE Boston/NNP)\n",
            "  for/IN\n",
            "  a/DT\n",
            "  conference/NN\n",
            "  ./.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4WoGDmgIPCrP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}